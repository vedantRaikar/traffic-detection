{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6419036,"sourceType":"datasetVersion","datasetId":3702648}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install ultralytics\nimport ultralytics\nultralytics.checks()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:48:26.181674Z","iopub.execute_input":"2024-08-15T04:48:26.182059Z","iopub.status.idle":"2024-08-15T04:48:45.710517Z","shell.execute_reply.started":"2024-08-15T04:48:26.182029Z","shell.execute_reply":"2024-08-15T04:48:45.709609Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nSetup complete ✅ (4 CPUs, 31.4 GB RAM, 5771.7/8062.4 GB disk)\n","output_type":"stream"}]},{"cell_type":"code","source":"!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:38:26.478124Z","iopub.execute_input":"2024-08-15T04:38:26.478910Z","iopub.status.idle":"2024-08-15T04:38:39.183961Z","shell.execute_reply.started":"2024-08-15T04:38:26.478882Z","shell.execute_reply":"2024-08-15T04:38:39.182802Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n100%|██████████████████████████████████████| 6.25M/6.25M [00:00<00:00, 21.1MB/s]\nUltralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nYOLOv8n summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n\nDownloading https://ultralytics.com/images/zidane.jpg to 'zidane.jpg'...\n100%|██████████████████████████████████████| 49.2k/49.2k [00:00<00:00, 1.13MB/s]\nimage 1/1 /kaggle/working/zidane.jpg: 384x640 2 persons, 1 tie, 106.7ms\nSpeed: 8.8ms preprocess, 106.7ms inference, 220.1ms postprocess per image at shape (1, 3, 384, 640)\nResults saved to \u001b[1mruns/detect/predict\u001b[0m\n💡 Learn more at https://docs.ultralytics.com/modes/predict\n","output_type":"stream"}]},{"cell_type":"code","source":"yaml_text = \"\"\"train: /kaggle/input/traffic-detection-project/train/images\nval: /kaggle/input/traffic-detection-project/valid/images\ntest: /kaggle/input/traffic-detection-project/test/test/images\n\nnc: 5\nnames: ['bicycle', 'bus', 'car', 'motorbike', 'person']\"\"\"\n\nwith open(\"/kaggle/working/data.yaml\", 'w') as file:\n    file.write(yaml_text)\n\n# To display the content of the file, you can use the 'cat' command like this:\n%cat /kaggle/working/data.yaml\n","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:49:08.052870Z","iopub.execute_input":"2024-08-15T04:49:08.053686Z","iopub.status.idle":"2024-08-15T04:49:09.064808Z","shell.execute_reply.started":"2024-08-15T04:49:08.053652Z","shell.execute_reply":"2024-08-15T04:49:09.063811Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"train: /kaggle/input/traffic-detection-project/train/images\nval: /kaggle/input/traffic-detection-project/valid/images\ntest: /kaggle/input/traffic-detection-project/test/test/images\n\nnc: 5\nnames: ['bicycle', 'bus', 'car', 'motorbike', 'person']","output_type":"stream"}]},{"cell_type":"code","source":"!yolo train model=yolov8n.pt data=/kaggle/working/data.yaml epochs=30 imgsz=448","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:20:22.658340Z","iopub.execute_input":"2024-08-15T05:20:22.659329Z","iopub.status.idle":"2024-08-15T05:45:04.096619Z","shell.execute_reply.started":"2024-08-15T05:20:22.659289Z","shell.execute_reply":"2024-08-15T05:45:04.095616Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/working/data.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=448, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train5\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nOverriding model.yaml nc=80 with nc=5\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \nModel summary: 225 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train5', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/traffic-detection-project/train/labels... 5805 ima\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/traffic-detection-project/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/traffic-detection-project/valid/labels... 549 images\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/traffic-detection-project/valid is not writeable, cache not saved.\nPlotting labels to runs/detect/train5/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\nImage sizes 448 train, 448 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train5\u001b[0m\nStarting training for 30 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/30      1.43G      1.481       1.71      1.076        156        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.693      0.555      0.612      0.379\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/30      1.46G      1.316      1.017      1.008        107        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270       0.73      0.608      0.675      0.421\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/30      1.45G      1.285     0.9396     0.9974        247        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.713      0.634      0.702      0.457\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/30      1.61G      1.251     0.8699     0.9859        224        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.741      0.689      0.737      0.482\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/30      1.43G      1.226     0.8354     0.9791        358        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.753      0.708      0.767      0.501\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/30      1.46G        1.2        0.8      0.971        292        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.761      0.682      0.759      0.503\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/30      1.37G      1.188      0.781     0.9672        209        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.785       0.68      0.759      0.495\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/30      1.45G      1.169     0.7565     0.9606        162        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.765      0.718      0.768      0.507\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/30      1.41G      1.153     0.7417     0.9535        327        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.818      0.733      0.798      0.533\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/30      1.56G      1.146     0.7243     0.9502        288        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.746      0.727      0.775      0.507\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      11/30       1.4G      1.137     0.7155     0.9489        124        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.839      0.736      0.805      0.545\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      12/30      1.48G      1.114     0.6953     0.9432        242        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.815      0.756      0.814      0.555\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      13/30      1.33G      1.107     0.6827     0.9393        261        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.811      0.768      0.826      0.565\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      14/30      1.52G      1.092     0.6723     0.9367        196        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.822      0.774      0.829      0.568\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      15/30      1.44G      1.093     0.6682     0.9352        217        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.825      0.758       0.83      0.571\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      16/30      1.42G      1.076     0.6553     0.9291        291        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.806      0.778      0.839      0.581\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      17/30       1.3G      1.073      0.649     0.9288        172        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.842      0.778      0.845      0.585\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      18/30      1.44G      1.055     0.6391      0.923        254        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.835      0.763      0.837      0.584\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      19/30      1.38G      1.046     0.6279     0.9216        325        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.828      0.786      0.846      0.593\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      20/30      1.36G      1.046     0.6272     0.9209        176        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.868      0.775      0.851      0.599\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      21/30      1.12G      1.006      0.588     0.9137        151        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.847       0.78      0.853      0.596\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      22/30      1.11G     0.9897     0.5734     0.9096        170        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.866      0.786      0.859      0.606\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      23/30      1.12G     0.9742     0.5631     0.9039        121        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.863      0.801      0.861      0.607\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      24/30      1.12G     0.9634     0.5517        0.9        154        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.869      0.806      0.868       0.62\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      25/30      1.12G     0.9506     0.5437     0.8979        128        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.881      0.793      0.868      0.619\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      26/30      1.12G      0.942     0.5368     0.8956        145        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.871      0.804      0.871      0.626\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      27/30      1.12G     0.9301     0.5303     0.8917        143        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.894       0.79      0.874       0.63\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      28/30      1.13G      0.916      0.519     0.8872        143        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.881      0.805      0.873      0.628\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      29/30      1.11G      0.908     0.5114     0.8858        107        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270       0.87      0.812      0.875      0.632\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      30/30      1.12G     0.8977     0.5072     0.8832        202        448: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.876      0.814      0.877      0.634\n\n30 epochs completed in 0.401 hours.\nOptimizer stripped from runs/detect/train5/weights/last.pt, 6.2MB\nOptimizer stripped from runs/detect/train5/weights/best.pt, 6.2MB\n\nValidating runs/detect/train5/weights/best.pt...\nUltralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 168 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        549       6270      0.875      0.814      0.877      0.634\n               bicycle        189        250      0.898      0.828      0.888      0.681\n                   bus         81        108      0.918      0.935      0.957      0.835\n                   car        520       3842      0.899      0.907      0.953      0.733\n             motorbike        331       1238      0.878      0.752      0.835      0.503\n                person        196        832      0.781      0.648      0.751      0.419\nSpeed: 0.3ms preprocess, 1.1ms inference, 0.0ms loss, 0.8ms postprocess per image\nResults saved to \u001b[1mruns/detect/train5\u001b[0m\n💡 Learn more at https://docs.ultralytics.com/modes/train\n","output_type":"stream"}]},{"cell_type":"code","source":"!yolo task=detect mode=predict  model=/kaggle/working/runs/detect/train3/weights/best.pt data=/kaggle/working/data.yaml epochs=2 imgsz=448","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:04:34.029339Z","iopub.execute_input":"2024-08-15T05:04:34.030067Z","iopub.status.idle":"2024-08-15T05:04:40.619860Z","shell.execute_reply.started":"2024-08-15T05:04:34.030032Z","shell.execute_reply":"2024-08-15T05:04:40.618876Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"WARNING ⚠️ 'source' argument is missing. Using default 'source=/opt/conda/lib/python3.10/site-packages/ultralytics/assets'.\nUltralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 168 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n\nimage 1/2 /opt/conda/lib/python3.10/site-packages/ultralytics/assets/bus.jpg: 448x352 1 bicycle, 2 persons, 53.8ms\nimage 2/2 /opt/conda/lib/python3.10/site-packages/ultralytics/assets/zidane.jpg: 256x448 2 cars, 51.4ms\nSpeed: 2.8ms preprocess, 52.6ms inference, 59.4ms postprocess per image at shape (1, 3, 256, 448)\nResults saved to \u001b[1mruns/detect/predict\u001b[0m\n💡 Learn more at https://docs.ultralytics.com/modes/predict\n","output_type":"stream"}]},{"cell_type":"code","source":"!yolo export model=yolov8n.pt format=onnx ","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:06:16.722282Z","iopub.execute_input":"2024-08-15T05:06:16.723149Z","iopub.status.idle":"2024-08-15T05:06:24.845143Z","shell.execute_reply.started":"2024-08-15T05:06:16.723083Z","shell.execute_reply":"2024-08-15T05:06:24.843924Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.0.0 CPU (Intel Xeon 2.00GHz)\nYOLOv8n summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n\n\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n\n\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n================ Diagnostic Run torch.onnx.export version 2.0.0 ================\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\n\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 0.8s, saved as 'yolov8n.onnx' (12.2 MB)\n\nExport complete (3.0s)\nResults saved to \u001b[1m/kaggle/working\u001b[0m\nPredict:         yolo predict task=detect model=yolov8n.onnx imgsz=640  \nValidate:        yolo val task=detect model=yolov8n.onnx imgsz=640 data=coco.yaml  \nVisualize:       https://netron.app\n💡 Learn more at https://docs.ultralytics.com/modes/export\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:07:22.002878Z","iopub.execute_input":"2024-08-15T05:07:22.003288Z","iopub.status.idle":"2024-08-15T05:07:22.007848Z","shell.execute_reply.started":"2024-08-15T05:07:22.003258Z","shell.execute_reply":"2024-08-15T05:07:22.006830Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!yolo export model=/kaggle/working/runs/detect/train3/weights/best.pt format=onnx --simplify ","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:07:41.060355Z","iopub.execute_input":"2024-08-15T05:07:41.061124Z","iopub.status.idle":"2024-08-15T05:08:02.179141Z","shell.execute_reply.started":"2024-08-15T05:07:41.061074Z","shell.execute_reply":"2024-08-15T05:08:02.177887Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"WARNING ⚠️ argument '--simplify' does not require leading dashes '--', updating to 'simplify'.\nUltralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.0.0 CPU (Intel Xeon 2.00GHz)\nModel summary (fused): 168 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n\n\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/kaggle/working/runs/detect/train3/weights/best.pt' with input shape (1, 3, 448, 448) BCHW and output shape(s) (1, 9, 4116) (5.9 MB)\n\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnxslim>=0.1.31', 'onnxruntime'] not found, attempting AutoUpdate...\nCollecting onnxslim>=0.1.31\n  Downloading onnxslim-0.1.32-py3-none-any.whl (130 kB)\n\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.5/130.5 kB 4.5 MB/s eta 0:00:00\n\u001b[?25hCollecting onnxruntime\n  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 94.3 MB/s eta 0:00:00\n\u001b[?25hRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from onnxslim>=0.1.31) (1.14.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxslim>=0.1.31) (1.12)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxslim>=0.1.31) (21.3)\nCollecting coloredlogs (from onnxruntime)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 175.6 MB/s eta 0:00:00\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (23.5.26)\nRequirement already satisfied: numpy<2.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.23.5)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (3.20.3)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 218.6 MB/s eta 0:00:00\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.10/site-packages (from onnx->onnxslim>=0.1.31) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxslim>=0.1.31) (3.0.9)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxslim>=0.1.31) (1.3.0)\nInstalling collected packages: humanfriendly, onnxslim, coloredlogs, onnxruntime\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.18.1 onnxslim-0.1.32\n\n\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 12.6s, installed 2 packages: ['onnxslim>=0.1.31', 'onnxruntime']\n\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n\n\n\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n================ Diagnostic Run torch.onnx.export version 2.0.0 ================\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\n\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.32...\n\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 13.9s, saved as '/kaggle/working/runs/detect/train3/weights/best.onnx' (11.6 MB)\n\nExport complete (16.0s)\nResults saved to \u001b[1m/kaggle/working/runs/detect/train3/weights\u001b[0m\nPredict:         yolo predict task=detect model=/kaggle/working/runs/detect/train3/weights/best.onnx imgsz=448  \nValidate:        yolo val task=detect model=/kaggle/working/runs/detect/train3/weights/best.onnx imgsz=448 data=/kaggle/working/data.yaml  \nVisualize:       https://netron.app\n💡 Learn more at https://docs.ultralytics.com/modes/export\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip3 install onnx>=1.10.0","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:08:15.351995Z","iopub.execute_input":"2024-08-15T05:08:15.352852Z","iopub.status.idle":"2024-08-15T05:08:26.912538Z","shell.execute_reply.started":"2024-08-15T05:08:15.352817Z","shell.execute_reply":"2024-08-15T05:08:26.911284Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"input_width = 448 \ninput_height = 448 \n\n!python3 /kaggle/working/runs/detect/train/weights/best.pt --img {input_height} {input_width} --batch 1 --include \"onnx\" --simplify","metadata":{"execution":{"iopub.status.busy":"2023-10-27T11:09:46.681575Z","iopub.execute_input":"2023-10-27T11:09:46.681962Z","iopub.status.idle":"2023-10-27T11:09:47.798499Z","shell.execute_reply.started":"2023-10-27T11:09:46.681931Z","shell.execute_reply":"2023-10-27T11:09:47.797418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to display images\nfrom IPython.display import Image, clear_output\nimport torch\nfrom yolov5 import utils\ndisplay = utils.notebook_init()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:03:46.953626Z","iopub.execute_input":"2023-10-27T09:03:46.954383Z","iopub.status.idle":"2023-10-27T09:03:57.950406Z","shell.execute_reply.started":"2023-10-27T09:03:46.954351Z","shell.execute_reply":"2023-10-27T09:03:57.949506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configure .yaml file to guide the model for training\nyaml_text = \"\"\"train: /kaggle/input/traffic-detection-project/train/images\nval: /kaggle/input/traffic-detection-project/valid/images\ntest: /kaggle/input/traffic-detection-project/test/test/images\n\nnc: 5\nnames: ['bicycle', 'bus', 'car', 'motorbike', 'person']\"\"\"\n\nwith open(\"data/data.yaml\", 'w') as file:\n    file.write(yaml_text)\n\n%cat data/data.yaml","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:04:07.091043Z","iopub.execute_input":"2023-10-27T09:04:07.091946Z","iopub.status.idle":"2023-10-27T09:04:08.057337Z","shell.execute_reply.started":"2023-10-27T09:04:07.091903Z","shell.execute_reply":"2023-10-27T09:04:08.056261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#customize iPython writefile so we can write variables\nfrom IPython.core.magic import register_line_cell_magic\n\n@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:51:14.637107Z","iopub.execute_input":"2023-10-04T14:51:14.637499Z","iopub.status.idle":"2023-10-04T14:51:14.643237Z","shell.execute_reply.started":"2023-10-04T14:51:14.637464Z","shell.execute_reply":"2023-10-04T14:51:14.642234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writetemplate models/custom_yolov5s.yaml\n# YOLOv5 🚀 by Ultralytics, AGPL-3.0 license\n\n# Parameters\nnc: 5  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\nanchors:\n  - [10,13, 16,30, 33,23]  # P3/8\n  - [30,61, 62,45, 59,119]  # P4/16\n  - [116,90, 156,198, 373,326]  # P5/32\n\n# YOLOv5 v6.0 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 9\n  ]\n\n# YOLOv5 v6.0 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n\n   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n  ]","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:51:14.644698Z","iopub.execute_input":"2023-10-04T14:51:14.645717Z","iopub.status.idle":"2023-10-04T14:51:14.656776Z","shell.execute_reply.started":"2023-10-04T14:51:14.645686Z","shell.execute_reply":"2023-10-04T14:51:14.655806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train yolov5s on custom data for 100 epochs\n# time its performance\n# we are going to using pre-trained weights from yolov5.pt model\nfrom datetime import datetime\nstart = datetime.now()\n!python train.py --img 448 --batch 32 --epochs 20 --data data/data.yaml --cfg models/custom_yolov5s.yaml --weights yolov5s.pt --name yolov5s_results  --cache\nend = datetime.now()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T14:51:14.658236Z","iopub.execute_input":"2023-10-04T14:51:14.658769Z","iopub.status.idle":"2023-10-04T15:23:41.860986Z","shell.execute_reply.started":"2023-10-04T14:51:14.65874Z","shell.execute_reply":"2023-10-04T15:23:41.859831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Runtime =\",end-start)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:23:41.862693Z","iopub.execute_input":"2023-10-04T15:23:41.863076Z","iopub.status.idle":"2023-10-04T15:23:41.869565Z","shell.execute_reply.started":"2023-10-04T15:23:41.863039Z","shell.execute_reply":"2023-10-04T15:23:41.868506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:29:07.648238Z","iopub.execute_input":"2023-10-04T15:29:07.648633Z","iopub.status.idle":"2023-10-04T15:29:07.653125Z","shell.execute_reply.started":"2023-10-04T15:29:07.648605Z","shell.execute_reply":"2023-10-04T15:29:07.652223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = plt.imread('runs/train/yolov5s_results/train_batch0.jpg')\nplt.figure(figsize=(20,15))\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:29:09.506977Z","iopub.execute_input":"2023-10-04T15:29:09.507321Z","iopub.status.idle":"2023-10-04T15:29:10.870349Z","shell.execute_reply.started":"2023-10-04T15:29:09.50729Z","shell.execute_reply":"2023-10-04T15:29:10.869212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python detect.py --source /kaggle/input/traffic-detection-project/test/images/ --weight runs/train/yolov5s_results/weights/best.pt --name expTestImage --conf 0.4","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:33:50.559637Z","iopub.execute_input":"2023-10-04T15:33:50.560009Z","iopub.status.idle":"2023-10-04T15:34:12.379513Z","shell.execute_reply.started":"2023-10-04T15:33:50.559976Z","shell.execute_reply":"2023-10-04T15:34:12.378345Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dict = {\n    'bicycle': (0, 255, 0),  # màu xanh lá cây\n    'bus': (0, 0, 255),      # màu xanh dương\n    'car': (255, 0, 0),      # màu đỏ\n    'motorbike': (255, 255, 0), # màu vàng\n    'person': (0, 255, 255)  # màu xanh lục\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:34:25.208238Z","iopub.execute_input":"2023-10-04T15:34:25.208669Z","iopub.status.idle":"2023-10-04T15:34:25.215488Z","shell.execute_reply.started":"2023-10-04T15:34:25.208624Z","shell.execute_reply":"2023-10-04T15:34:25.213962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/yolov5/runs/detect/expTestImage","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:34:26.995882Z","iopub.execute_input":"2023-10-04T15:34:26.996218Z","iopub.status.idle":"2023-10-04T15:34:27.959579Z","shell.execute_reply.started":"2023-10-04T15:34:26.996192Z","shell.execute_reply":"2023-10-04T15:34:27.958294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom glob import glob\nall_path = glob(\"/kaggle/working/yolov5/runs/detect/expTestImage2/*\")\nImage.open(all_path[1])","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:36:10.913335Z","iopub.execute_input":"2023-10-04T15:36:10.914432Z","iopub.status.idle":"2023-10-04T15:36:11.080575Z","shell.execute_reply.started":"2023-10-04T15:36:10.914385Z","shell.execute_reply":"2023-10-04T15:36:11.079804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install onnx>=1.10.0","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:36:58.864714Z","iopub.execute_input":"2023-10-04T15:36:58.865076Z","iopub.status.idle":"2023-10-04T15:37:08.391596Z","shell.execute_reply.started":"2023-10-04T15:36:58.865047Z","shell.execute_reply":"2023-10-04T15:37:08.390261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_width = 448 \ninput_height = 448 \n\n!python3 export.py --weights runs/train/yolov5s_results/weights/best.pt --img {input_height} {input_width} --batch 1 --include \"onnx\" --simplify","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:47:11.890534Z","iopub.execute_input":"2023-10-04T15:47:11.890892Z","iopub.status.idle":"2023-10-04T15:47:20.821842Z","shell.execute_reply.started":"2023-10-04T15:47:11.890866Z","shell.execute_reply":"2023-10-04T15:47:20.820635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!pip install -U yolov5\n","metadata":{}},{"cell_type":"code","source":"train: /kaggle/input/traffic-detection-project/train/images\nval: /kaggle/input/traffic-detection-project/valid/images","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:10:41.247382Z","iopub.execute_input":"2024-08-15T04:10:41.248230Z","iopub.status.idle":"2024-08-15T04:10:41.253960Z","shell.execute_reply.started":"2024-08-15T04:10:41.248197Z","shell.execute_reply":"2024-08-15T04:10:41.252752Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    train: /kaggle/input/traffic-detection-project/train/images\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (529023119.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}